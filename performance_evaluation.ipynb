{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1155,"status":"ok","timestamp":1743601011172,"user":{"displayName":"Luca Fossen","userId":"05839406467121902627"},"user_tz":-120},"id":"yuSUYsE1GY-x","outputId":"b6560b14-1210-4abf-daca-edec62cc40d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Othercomputers/lucas-yoga/Current/INFO381/code/INFO381-GitHub\n"]}],"source":["try:\n","    # Comment out if not using colab\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Specific for luca's computer\n","    %cd \"/content/drive/Othercomputers/lucas-yoga/Current/INFO381/code/INFO381-GitHub\"\n","    using_colab = True\n","except:\n","    print(\"Not using Google Colab\")\n","    using_colab = False"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4578,"status":"ok","timestamp":1743601015751,"user":{"displayName":"Luca Fossen","userId":"05839406467121902627"},"user_tz":-120},"id":"L9QeOJBoHp-l","outputId":"ab81fca9-db33-42fc-ed23-9077abe0fca0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-2v49vero\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-2v49vero\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.0)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.0)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.1.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"QnQrRfowGY-y","executionInfo":{"status":"ok","timestamp":1743601020318,"user_tz":-120,"elapsed":4560,"user":{"displayName":"Luca Fossen","userId":"05839406467121902627"}}},"outputs":[],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torchvision.models as models\n","import torch.nn as nn\n","from sklearn.metrics import accuracy_score\n","import clip\n","\n","\n","from utils import get_dataloaders\n","from model_definitions import CLIPClassifier"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":264958,"status":"ok","timestamp":1743602787490,"user":{"displayName":"Luca Fossen","userId":"05839406467121902627"},"user_tz":-120},"id":"0EKg-zu1GY-1","outputId":"c1fe55b2-08a8-4d26-f7c5-7fe81a7ba35a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in Google Colab\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["ResNet18 Train Accuracy: 0.79\n","ResNet18 Test Accuracy: 0.79\n"]}],"source":["# Define resize transform for the CNN\n","resize_transform = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.ToTensor()\n","])\n","\n","# Load both train and test dataloaders for the CNN model\n","cnn_train_loader, cnn_test_loader = get_dataloaders(\n","    zip_path=\"fake_vs_real.zip\",\n","    batch_size=32,\n","    split='both',\n","    transform=resize_transform\n",")\n","\n","# Setup the ResNet18 model\n","cnn_model = models.resnet18(pretrained=False)\n","num_ftrs = cnn_model.fc.in_features\n","cnn_model.fc = nn.Linear(num_ftrs, 2)\n","cnn_model.load_state_dict(torch.load(\"models/resnet18_cnn.pth\"))\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","cnn_model.to(device)\n","cnn_model.eval()\n","\n","# Define evaluation function (same as for CLIP)\n","def evaluate(model, dataloader):\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return accuracy\n","\n","# Evaluate on train and test sets\n","accuracy_train_cnn = evaluate(cnn_model, cnn_train_loader)\n","accuracy_test_cnn = evaluate(cnn_model, cnn_test_loader)\n","\n","print(f\"ResNet18 Train Accuracy: {accuracy_train_cnn:.2f}\")\n","print(f\"ResNet18 Test Accuracy: {accuracy_test_cnn:.2f}\")\n"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":269820,"status":"ok","timestamp":1743602522528,"user":{"displayName":"Luca Fossen","userId":"05839406467121902627"},"user_tz":-120},"id":"zi5M8BgyGY-1","outputId":"956d4b95-fe8b-403a-e78a-3dd290e6ef9e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Running in Google Colab\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1045: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["CLIP ViT Train Accuracy: 0.97\n","CLIP ViT Test Accuracy: 0.95\n"]}],"source":["# 1) Load the CLIP backbone and our classifier model\n","clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","\n","# Load both train and test dataloaders\n","clip_train_loader, clip_test_loader = get_dataloaders(zip_path=\"fake_vs_real.zip\", batch_size=32, split='both', transform=preprocess)\n","\n","model = CLIPClassifier(clip_model, embed_dim=512, num_classes=2).to(device)\n","model.load_state_dict(torch.load(\"models/clip_classifier_10epochs.pth\", map_location=device))\n","model.eval()\n","\n","# 2) Define an evaluation function\n","from sklearn.metrics import accuracy_score\n","\n","def evaluate(model, dataloader):\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    return accuracy\n","\n","# 3) Evaluate on both sets\n","accuracy_train = evaluate(model, clip_train_loader)\n","accuracy_test = evaluate(model, clip_test_loader)\n","\n","print(f\"CLIP ViT Train Accuracy: {accuracy_train:.2f}\")\n","print(f\"CLIP ViT Test Accuracy: {accuracy_test:.2f}\")\n"]},{"cell_type":"code","source":[],"metadata":{"id":"_GtTzpqZtQEB","executionInfo":{"status":"aborted","timestamp":1743601101098,"user_tz":-120,"elapsed":91248,"user":{"displayName":"Luca Fossen","userId":"05839406467121902627"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"interpreter":{"hash":"831be88b21373d988d28660f54178f22fb70e2a90418832270cb802719345912"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}