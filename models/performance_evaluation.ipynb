{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1833,"status":"ok","timestamp":1746088427334,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"yuSUYsE1GY-x","outputId":"f1fd8386-b192-4d24-c5b8-6a6fe3a57a31"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","/content/drive/Othercomputers/Min MacBook Pro/INFO381-GitHub\n"]}],"source":["try:\n","    # Comment out if not using colab\n","    from google.colab import drive\n","    drive.mount('/content/drive')\n","\n","    # Specific for luca's computer\n","    %cd \"/content/drive/Othercomputers/Min MacBook Pro/INFO381-GitHub\"\n","    using_colab = True\n","except:\n","    print(\"Not using Google Colab\")\n","    using_colab = False"]},{"cell_type":"markdown","metadata":{},"source":["**To run code with CLIP, either have git install or run on Google Colab**"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4785,"status":"ok","timestamp":1746088432121,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"L9QeOJBoHp-l","outputId":"ddb9786a-52b4-46d7-afda-61b5c6f020d2"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting git+https://github.com/openai/CLIP.git\n","  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-v73_x3bm\n","  Running command git clone --filter=blob:none --quiet https://github.com/openai/CLIP.git /tmp/pip-req-build-v73_x3bm\n","  Resolved https://github.com/openai/CLIP.git to commit dcba3cb2e2827b402d2701e7e1c7d9fed8a20ef1\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: ftfy in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (6.3.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (24.2)\n","Requirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2024.11.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (4.67.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from clip==1.0) (0.21.0+cu124)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.11/dist-packages (from ftfy->clip==1.0) (0.2.13)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (4.13.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->clip==1.0) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->clip==1.0) (1.3.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (2.0.2)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision->clip==1.0) (11.2.1)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->clip==1.0) (3.0.2)\n"]}],"source":["!pip install git+https://github.com/openai/CLIP.git"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":5324,"status":"ok","timestamp":1746088437447,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"QnQrRfowGY-y"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/ot/opt/anaconda3/lib/python3.9/site-packages/pandas/core/computation/expressions.py:21: UserWarning: Pandas requires version '2.8.4' or newer of 'numexpr' (version '2.8.3' currently installed).\n","  from pandas.core.computation.check import NUMEXPR_INSTALLED\n","/Users/ot/opt/anaconda3/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n","  from pandas.core import (\n"]}],"source":["import torch\n","import torchvision.transforms as transforms\n","from torch.utils.data import DataLoader\n","from torchvision.datasets import ImageFolder\n","import torchvision.models as models\n","import torch.nn as nn\n","from sklearn.metrics import accuracy_score, classification_report\n","import clip\n","\n","\n","# Local imports\n","import sys\n","import os\n","\n","if using_colab:\n","    sys.path.append(os.path.abspath(\"helper_functions\"))\n","else:\n","    sys.path.append(os.path.abspath(\"../helper_functions\"))\n","from utils import get_dataloaders\n","from model_definitions import CLIPClassifier"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"vLawMf_q4PX8"},"source":["**Load both train and test dataloaders for the CNN model**"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21478,"status":"ok","timestamp":1746088458924,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"0EKg-zu1GY-1","outputId":"2d7c3afa-d490-405f-ecea-7412e52026d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running in Google Colab\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n","  warnings.warn(msg)\n"]},{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Define resize transform for the CNN\n","resize_transform = transforms.Compose([\n","    transforms.Resize((512, 512)),\n","    transforms.ToTensor()\n","])\n","\n","cnn_train_loader, cnn_test_loader = get_dataloaders(\n","    zip_path=\"fake_vs_real.zip\",\n","    batch_size=32,\n","    split='both',\n","    transform=resize_transform\n",")\n","\n","cnn_model = models.resnet18(pretrained=False)\n","num_ftrs = cnn_model.fc.in_features\n","cnn_model.fc = nn.Linear(num_ftrs, 2)\n","cnn_model.load_state_dict(torch.load(\"models/resnet18_cnn.pth\"))\n","\n","cnn_model.to(device)\n","cnn_model.eval()"]},{"cell_type":"markdown","metadata":{"id":"zRHazBYM4DN4"},"source":["**Load the CLIP backbone and our classifier model**"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24841,"status":"ok","timestamp":1746088483771,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"zi5M8BgyGY-1","outputId":"bc6bfc33-3480-43f5-db6d-0222e9e9d496"},"outputs":[{"name":"stdout","output_type":"stream","text":["Running in Google Colab\n"]},{"data":{"text/plain":["CLIPClassifier(\n","  (clip_model): CLIP(\n","    (visual): VisionTransformer(\n","      (conv1): Conv2d(3, 768, kernel_size=(32, 32), stride=(32, 32), bias=False)\n","      (ln_pre): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (transformer): Transformer(\n","        (resblocks): Sequential(\n","          (0): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (1): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (2): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (3): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (4): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (5): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (6): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (7): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (8): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (9): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (10): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","          (11): ResidualAttentionBlock(\n","            (attn): MultiheadAttention(\n","              (out_proj): NonDynamicallyQuantizableLinear(in_features=768, out_features=768, bias=True)\n","            )\n","            (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (mlp): Sequential(\n","              (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n","              (gelu): QuickGELU()\n","              (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n","            )\n","            (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","          )\n","        )\n","      )\n","      (ln_post): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","    )\n","    (transformer): Transformer(\n","      (resblocks): Sequential(\n","        (0): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (1): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (2): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (3): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (4): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (5): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (6): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (7): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (8): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (9): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (10): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","        (11): ResidualAttentionBlock(\n","          (attn): MultiheadAttention(\n","            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n","          )\n","          (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","          (mlp): Sequential(\n","            (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n","            (gelu): QuickGELU()\n","            (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n","          )\n","          (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","        )\n","      )\n","    )\n","    (token_embedding): Embedding(49408, 512)\n","    (ln_final): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n","  )\n","  (classifier): Linear(in_features=512, out_features=2, bias=True)\n",")"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["clip_model, preprocess = clip.load(\"ViT-B/32\", device=device)\n","clip_classifier = CLIPClassifier(clip_model, embed_dim=512, num_classes=2).to(device)\n","clip_classifier.load_state_dict(torch.load(\"models/clip_classifier_10epochs.pth\", map_location=device))\n","clip_train_loader, clip_test_loader = get_dataloaders(zip_path=\"fake_vs_real.zip\", batch_size=32, split='both', transform=preprocess)\n","clip_classifier.eval()"]},{"cell_type":"markdown","metadata":{"id":"jYLIvu-o4oeJ"},"source":["**Define evaluation function (Works for both models)**"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1746089346250,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"5DsmeV9v3ZX0"},"outputs":[],"source":["def evaluate(model, dataloader):\n","    all_preds, all_labels = [], []\n","\n","    with torch.no_grad():\n","        for inputs, labels in dataloader:\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(labels.cpu().numpy())\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","\n","    return accuracy, all_preds, all_labels"]},{"cell_type":"markdown","metadata":{"id":"Ey_iY0gO_ZYe"},"source":["**Evaluate accuracy on train and test sets on CNN**"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":252631,"status":"ok","timestamp":1746089077813,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"PiHzExBr4Lkr","outputId":"bd9ea705-29b1-4939-ff34-113e17d3c46d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["ResNet18 Train Accuracy: 0.94\n","ResNet18 Test Accuracy: 0.89\n"]}],"source":["accuracy_train_cnn, preds_train_cnn, labels_train_cnn = evaluate(cnn_model, cnn_train_loader)\n","accuracy_test_cnn, preds_test_cnn, labels_test_cnn = evaluate(cnn_model, cnn_test_loader)\n","\n","print(f\"ResNet18 Train Accuracy: {accuracy_train_cnn:.2f}\")\n","print(f\"ResNet18 Test Accuracy: {accuracy_test_cnn:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"YIUi7kTD_frJ"},"source":["**Evaluate accuracy on train and test sets on CLIP**"]},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":259558,"status":"ok","timestamp":1746089633403,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"_GtTzpqZtQEB","outputId":"541be3d8-e2e9-44fd-b7ca-d6fb30e58cd5"},"outputs":[{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n","/usr/local/lib/python3.11/dist-packages/PIL/Image.py:1043: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["CLIP ViT Train Accuracy: 0.97\n","CLIP ViT Test Accuracy: 0.95\n"]}],"source":["# 3) Evaluate on both sets\n","accuracy_train_clip, preds_train_clip, labels_train_clip = evaluate(clip_classifier, clip_train_loader)\n","accuracy_test_clip, preds_test_clip, labels_test_clip = evaluate(clip_classifier, clip_test_loader)\n","\n","print(f\"CLIP ViT Train Accuracy: {accuracy_train_clip:.2f}\")\n","print(f\"CLIP ViT Test Accuracy: {accuracy_test_clip:.2f}\")\n"]},{"cell_type":"markdown","metadata":{"id":"DMF7LwrW_lMO"},"source":["**Evaluate metrics on train and test sets on CNN**"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":48,"status":"ok","timestamp":1746089648683,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"0tpJyrAPkqyg","outputId":"69ec8cfc-db6a-4f25-d446-ca449a58c8e3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Classification Report (Train):\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.92      0.94      3024\n","           1       0.92      0.96      0.94      3023\n","\n","    accuracy                           0.94      6047\n","   macro avg       0.94      0.94      0.94      6047\n","weighted avg       0.94      0.94      0.94      6047\n","\n","Classification Report (Test):\n","              precision    recall  f1-score   support\n","\n","           0       0.91      0.87      0.89       756\n","           1       0.88      0.92      0.90       757\n","\n","    accuracy                           0.89      1513\n","   macro avg       0.89      0.89      0.89      1513\n","weighted avg       0.89      0.89      0.89      1513\n","\n"]}],"source":["print(\"Classification Report (Train):\")\n","print(classification_report(labels_train_cnn, preds_train_cnn))\n","\n","print(\"Classification Report (Test):\")\n","print(classification_report(labels_test_cnn, preds_test_cnn))"]},{"cell_type":"markdown","metadata":{"id":"f3QYRN8J_zhy"},"source":["**Evaluate metrics on train and test sets on CLIP**"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22,"status":"ok","timestamp":1746089651192,"user":{"displayName":"ole thomas Petrusson","userId":"02182860559771238641"},"user_tz":-120},"id":"vBu79rGY_R1w","outputId":"33e23739-1899-412d-a425-09f7e2d0dd08"},"outputs":[{"name":"stdout","output_type":"stream","text":["CLIP ViT Classification Report (Train):\n","              precision    recall  f1-score   support\n","\n","           0       0.96      0.97      0.97      3024\n","           1       0.97      0.96      0.96      3023\n","\n","    accuracy                           0.97      6047\n","   macro avg       0.97      0.97      0.97      6047\n","weighted avg       0.97      0.97      0.97      6047\n","\n","CLIP ViT Classification Report (Test):\n","              precision    recall  f1-score   support\n","\n","           0       0.94      0.95      0.95       756\n","           1       0.95      0.94      0.95       757\n","\n","    accuracy                           0.95      1513\n","   macro avg       0.95      0.95      0.95      1513\n","weighted avg       0.95      0.95      0.95      1513\n","\n"]}],"source":["print(\"CLIP ViT Classification Report (Train):\")\n","print(classification_report(labels_train_clip, preds_train_clip))\n","\n","print(\"CLIP ViT Classification Report (Test):\")\n","print(classification_report(labels_test_clip, preds_test_clip))"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","provenance":[]},"interpreter":{"hash":"831be88b21373d988d28660f54178f22fb70e2a90418832270cb802719345912"},"kernelspec":{"display_name":"Python 3.9.13 64-bit ('base': conda)","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
